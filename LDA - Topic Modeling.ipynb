{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n",
    "- Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "- LDA ssumes that every chunk of text we feed into it will contain words that are somehow related.\n",
    "- It also assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we'll use here is a list of over one million news headlines published over a period of 15 years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv('~/abcnews-data.csv')\n",
    "\n",
    "data_text = data[:300000][['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "\n",
    "documents = data_text\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tokenization**: gensim.utils.simple_preprocess\n",
    "- **Stopwords**: inlcuding stopwords and those less than 3 characters\n",
    "- **Lemmatized**: Verbs in past or funture tenses are changed into present. Words in third person are changed to first person.\n",
    "- **Stemmed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim** is a python library for topic modelling, document indexing and similarity retrieval with large corpora.\n",
    "\n",
    "**genism.utils**: generate various utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **WordNet** is a part of python's Natural Language Toolkit. It is a large word database of English Nouns, Adjective, Adverbs and Verbs. \n",
    "\n",
    "**Lemmatization** is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just remove the last few characters. \n",
    "\n",
    "There are several pythons packages available to implement lemmatization, such as:\n",
    "1. **WordNet Lemmatizer**: is one of the earliest and most commonly used lemmatizers. We have to down it first in order to use it(as the previous step). And usually **POS** (part-of-speech) should be provided as the argument to imporve its accuracy rate.\n",
    "\n",
    "\n",
    "2. **TextBlob**: is a powerful, fast and convenient NLP packages. Using the Word and TextBlob objects, its quite straighforward to parse and lemmatize words and sentences respectively. POS also needed.\n",
    "...\n",
    "\n",
    "**SnowballStemmer** is a stemming algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\xiaoj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to first lemmatize, then stem the text\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos = 'v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['rain', 'helps', 'dampen', 'bushfires']\n",
      "\n",
      "\n",
      "Tokenized, Lemmatized and Stemmed documents: \n",
      "['rain', 'help', 'dampen', 'bushfir']\n"
     ]
    }
   ],
   "source": [
    "# Preview a document after preprocessing\n",
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('Original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "\n",
    "print('\\n\\nTokenized, Lemmatized and Stemmed documents: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [decid, communiti, broadcast, licenc]\n",
       "1                        [wit, awar, defam]\n",
       "2    [call, infrastructur, protect, summit]\n",
       "3               [staff, aust, strike, rise]\n",
       "4      [strike, affect, australian, travel]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess(token, lemmatize, stem) the whole document text \n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Convert Text into numeric vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine can't process text data in raw form. They need us to break down the text into a numerical format that's easily readable by the machine.\n",
    "\n",
    "Both Bag-of-words(**BOW**) and **TF-IDF** are techniques that help us convert text sentences into numeric vectors.\n",
    "\n",
    "**BOW**:First build a vocabulary from all the unique words; then take each of these words and mark their occurrence in each documents.\n",
    "\n",
    "    - If the new sentences contain new words, vocabulary size would increase, the lenght of the vectors would increase too;\n",
    "    - The vectors would contain many 0s, which result in a sparse matrix;\n",
    "    - We are retaining no information on the grammar of the sentences nor on the ordering of the words in the text.\n",
    "\n",
    "**TF-IDF**: give larger values for less requent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **gensim.corpora.Dictionary()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets' create a dictionary from 'processed_docs' containing the number of times a word appears in the training set. To do that, let's use gensim.corpora.Dictionary()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from 'processed_docs' containing the number of times a word appears\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "# Check dictionary created\n",
    "count = 0\n",
    "for k, v in dictionary.items():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim **filter_extremes(no_below = ?, no_above = ?, keep_n =? )**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means to filter out tokens that appear in:\n",
    "- less than no_below documents\n",
    "- more than no_above documents(fraction of total corpus size)\n",
    "- after the above two steps, keep only the first keep_n most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the above dictionary filter_extremes() \n",
    "dictionary.filter_extremes(no_below = 15, no_above = 0.1, keep_n = 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **gensim.corpora.Dictionary.doc2bow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert document into the bag-of-words(**BOW**) format.(list of token_id, token_count) Each word is assumed to be a tokenized and normalized string. **Apply tokenization, stemming etc. before calling this method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the BOW model for each document.\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 71(\"bushfir\")appears 1 time.\n",
      "Word 107(\"help\")appears 1 time.\n",
      "Word 462(\"rain\")appears 1 time.\n",
      "Word 3530(\"dampen\")appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# preview BOW for some sample\n",
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print('Word {}(\\\"{}\")appears {} time.'.format(bow_doc_4310[i][0],\n",
    "                                                 dictionary[bow_doc_4310[i][0]],\n",
    "                                                 bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setp 3.2 TF-IDF on our document set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While performing TF-IDF on the corpus is not necessary for LDA implement using the gensim model, it is recemmened. TF-IDF expects a bag-of-words traning corpus during initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF stands for \"Term Frequency, Inverse Document Frequency\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is a way to score the importance of words in a document based on how frequently they appear across multiple documents.\n",
    "- If a word appears frequently in a document, it is import and a high score will be given to this word. But if a word appears in many documents, it's not a unique identifier and a low score will be given to this word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TF(w)** = (Number of times term w appears in a document) / (Total number of terms in the document)\n",
    "- **IDF(w)** = log_e(Total number of documents / Number of documents with term w in it)\n",
    "- **TF-IDF = TF * IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf model object using models.TfidModel on 'bow_corpus' \n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformation to the entire corpus\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5959813347777092),\n",
      " (1, 0.39204529549491984),\n",
      " (2, 0.48531419274988147),\n",
      " (3, 0.5055461098578569)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Preview TF-IDF scores for first document\n",
    "\"\"\"\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going for 10 topics in the document corpus.\n",
    "\n",
    "Some of parameters we will be tweaking are:\n",
    "- **number_topic**: is the number of requested latent topics to be extracted from the training corpus.\n",
    "\n",
    "- **id2word**: is a mapping from word ids(integers) to words(strings).\n",
    "\n",
    "- **workers**：is the number of extra processes to use for parallelization.Uses all available cores by default.\n",
    "\n",
    "- **alpha** and **eta** are hyperparameters that affect sparsity of the document-topic(theta) and topic-word(lambda) distribution. We will let these be the default values for now (default value is 1/num_topics)\n",
    "\n",
    "\n",
    "    - Alpha is the per document topic distribution.\n",
    "        - High alpha: Every document has a mixture of all topics\n",
    "        - Low alpha: Every document has a mixture of very few topics\n",
    "    \n",
    "    - Eta is the per topic word distribution\n",
    "        - High eta: Each topic has a mixture of most words\n",
    "        - Low eta: Each topic has a mixture of few words\n",
    "\n",
    "- **passes**: is the number of training passes through the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.023*\"closer\" + 0.018*\"talk\" + 0.016*\"market\" + 0.016*\"deal\" + 0.014*\"nuclear\" + 0.012*\"firefight\" + 0.011*\"trade\" + 0.011*\"year\" + 0.011*\"bush\" + 0.010*\"iran\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.018*\"open\" + 0.015*\"boost\" + 0.014*\"rain\" + 0.013*\"stand\" + 0.012*\"action\" + 0.012*\"centr\" + 0.012*\"worker\" + 0.012*\"campaign\" + 0.011*\"howard\" + 0.011*\"fall\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.037*\"charg\" + 0.032*\"court\" + 0.030*\"face\" + 0.020*\"jail\" + 0.020*\"accus\" + 0.019*\"murder\" + 0.018*\"drug\" + 0.015*\"polic\" + 0.014*\"case\" + 0.014*\"public\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.038*\"crash\" + 0.026*\"investig\" + 0.023*\"polic\" + 0.017*\"die\" + 0.015*\"victim\" + 0.013*\"dead\" + 0.013*\"leav\" + 0.013*\"train\" + 0.012*\"shoot\" + 0.011*\"famili\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.020*\"forc\" + 0.019*\"lead\" + 0.015*\"win\" + 0.014*\"take\" + 0.014*\"world\" + 0.013*\"troop\" + 0.013*\"final\" + 0.013*\"play\" + 0.011*\"storm\" + 0.010*\"aussi\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.032*\"council\" + 0.019*\"health\" + 0.019*\"minist\" + 0.019*\"servic\" + 0.015*\"opposit\" + 0.015*\"govt\" + 0.014*\"coast\" + 0.014*\"break\" + 0.013*\"plan\" + 0.012*\"indigen\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.044*\"water\" + 0.032*\"plan\" + 0.018*\"council\" + 0.016*\"concern\" + 0.014*\"fear\" + 0.013*\"resid\" + 0.013*\"group\" + 0.011*\"green\" + 0.011*\"hous\" + 0.011*\"nation\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.054*\"govt\" + 0.040*\"urg\" + 0.024*\"help\" + 0.023*\"call\" + 0.016*\"fund\" + 0.015*\"farmer\" + 0.014*\"fight\" + 0.012*\"inquiri\" + 0.012*\"law\" + 0.011*\"polic\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.047*\"polic\" + 0.029*\"kill\" + 0.026*\"miss\" + 0.024*\"attack\" + 0.023*\"warn\" + 0.023*\"continu\" + 0.018*\"search\" + 0.017*\"driver\" + 0.014*\"iraq\" + 0.014*\"expect\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.019*\"labor\" + 0.016*\"rise\" + 0.016*\"govt\" + 0.013*\"say\" + 0.012*\"road\" + 0.011*\"sale\" + 0.010*\"state\" + 0.010*\"rudd\" + 0.009*\"busi\" + 0.009*\"urg\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model using gensim.models.LdaMulticore and save it to 'ida_model'\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                      num_topics = 10,\n",
    "                                      id2word = dictionary,\n",
    "                                      passes = 2,\n",
    "                                      workers = 2)\n",
    "\n",
    "# For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print('Topic: {} \\nWords: {}'.format(idx,topic))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.2 Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.008*\"bird\" + 0.007*\"test\" + 0.006*\"england\" + 0.006*\"drug\" + 0.006*\"boat\" + 0.005*\"illeg\" + 0.005*\"kangaroo\" + 0.005*\"pakistan\" + 0.005*\"lanka\" + 0.005*\"polic\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Word: 0.023*\"crash\" + 0.022*\"polic\" + 0.013*\"miss\" + 0.013*\"search\" + 0.012*\"investig\" + 0.010*\"die\" + 0.010*\"road\" + 0.009*\"fatal\" + 0.008*\"driver\" + 0.008*\"accid\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Word: 0.027*\"closer\" + 0.017*\"charg\" + 0.014*\"court\" + 0.011*\"face\" + 0.011*\"jail\" + 0.010*\"assault\" + 0.010*\"polic\" + 0.009*\"murder\" + 0.009*\"kill\" + 0.009*\"blaze\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Word: 0.016*\"water\" + 0.009*\"govt\" + 0.008*\"fund\" + 0.007*\"plan\" + 0.007*\"council\" + 0.007*\"boost\" + 0.006*\"rain\" + 0.006*\"urg\" + 0.006*\"coast\" + 0.006*\"drought\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Word: 0.007*\"teacher\" + 0.006*\"strike\" + 0.006*\"union\" + 0.006*\"isra\" + 0.006*\"govt\" + 0.006*\"palestinian\" + 0.006*\"kill\" + 0.006*\"israel\" + 0.006*\"beazley\" + 0.005*\"concern\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Word: 0.008*\"shortag\" + 0.007*\"retir\" + 0.005*\"murray\" + 0.005*\"staff\" + 0.005*\"season\" + 0.004*\"record\" + 0.004*\"villag\" + 0.004*\"success\" + 0.004*\"darl\" + 0.004*\"forc\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Word: 0.011*\"price\" + 0.010*\"market\" + 0.008*\"bushfir\" + 0.007*\"rise\" + 0.007*\"nuclear\" + 0.006*\"plan\" + 0.006*\"govt\" + 0.006*\"rat\" + 0.005*\"bail\" + 0.005*\"council\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Word: 0.008*\"hick\" + 0.007*\"farm\" + 0.007*\"grower\" + 0.007*\"speed\" + 0.007*\"doubt\" + 0.007*\"wind\" + 0.007*\"govt\" + 0.005*\"appoint\" + 0.005*\"merger\" + 0.005*\"downer\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Word: 0.008*\"open\" + 0.007*\"final\" + 0.007*\"troop\" + 0.006*\"tiger\" + 0.006*\"aussi\" + 0.006*\"win\" + 0.006*\"solomon\" + 0.006*\"australia\" + 0.005*\"uranium\" + 0.005*\"black\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Word: 0.009*\"rudd\" + 0.008*\"govt\" + 0.007*\"develop\" + 0.007*\"guilti\" + 0.006*\"care\" + 0.006*\"climat\" + 0.006*\"chang\" + 0.006*\"council\" + 0.005*\"truck\" + 0.005*\"cancer\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define lda model using corpus_tfdif\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n",
    "                                            num_topics = 10,\n",
    "                                            id2word = dictionary,\n",
    "                                            passes = 2,\n",
    "                                            workers = 4)\n",
    "\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics():\n",
    "    print(\"Topic: {} \\nWord: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, when using tf-idf, heavier weights are given to words that are not as frequent which results in nouns being factored in. That makes it harder to figure out the categories as nouns can be hard to categorize. This goes to show that the models we apply depend on the type of corpus of text we are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Performance evaluation by classifying sample document using LDA BOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4867934584617615\t \n",
      "Topic: 0.025*\"council\" + 0.017*\"rise\" + 0.017*\"price\" + 0.016*\"centr\" + 0.013*\"land\" + 0.013*\"mayor\" + 0.013*\"plan\" + 0.012*\"prompt\" + 0.011*\"studi\" + 0.010*\"bushfir\"\n",
      "\n",
      "Score: 0.3531844913959503\t \n",
      "Topic: 0.035*\"water\" + 0.019*\"servic\" + 0.018*\"farmer\" + 0.018*\"drought\" + 0.014*\"break\" + 0.012*\"nation\" + 0.012*\"rain\" + 0.012*\"park\" + 0.011*\"help\" + 0.011*\"health\"\n",
      "\n",
      "Score: 0.020003773272037506\t \n",
      "Topic: 0.059*\"govt\" + 0.027*\"urg\" + 0.023*\"plan\" + 0.022*\"fund\" + 0.020*\"council\" + 0.016*\"group\" + 0.013*\"closer\" + 0.012*\"consid\" + 0.012*\"boost\" + 0.012*\"defend\"\n",
      "\n",
      "Score: 0.020003309473395348\t \n",
      "Topic: 0.074*\"polic\" + 0.031*\"charg\" + 0.027*\"court\" + 0.026*\"face\" + 0.022*\"crash\" + 0.020*\"investig\" + 0.019*\"death\" + 0.019*\"miss\" + 0.018*\"jail\" + 0.016*\"murder\"\n",
      "\n",
      "Score: 0.02000252529978752\t \n",
      "Topic: 0.030*\"hospit\" + 0.029*\"hous\" + 0.022*\"sydney\" + 0.020*\"blaze\" + 0.017*\"leader\" + 0.015*\"firefight\" + 0.014*\"home\" + 0.013*\"blame\" + 0.013*\"hick\" + 0.012*\"drink\"\n",
      "\n",
      "Score: 0.020002499222755432\t \n",
      "Topic: 0.015*\"labor\" + 0.014*\"deal\" + 0.014*\"look\" + 0.013*\"south\" + 0.013*\"power\" + 0.013*\"timor\" + 0.012*\"protest\" + 0.012*\"wont\" + 0.011*\"terror\" + 0.011*\"east\"\n",
      "\n",
      "Score: 0.020002495497465134\t \n",
      "Topic: 0.038*\"kill\" + 0.026*\"iraq\" + 0.026*\"attack\" + 0.014*\"forc\" + 0.014*\"troop\" + 0.014*\"bomb\" + 0.013*\"dead\" + 0.012*\"australian\" + 0.010*\"injur\" + 0.009*\"iraqi\"\n",
      "\n",
      "Score: 0.020002495497465134\t \n",
      "Topic: 0.021*\"elect\" + 0.015*\"die\" + 0.015*\"lose\" + 0.012*\"campaign\" + 0.009*\"rescu\" + 0.009*\"link\" + 0.009*\"say\" + 0.008*\"doubt\" + 0.008*\"cancer\" + 0.008*\"pledg\"\n",
      "\n",
      "Score: 0.020002495497465134\t \n",
      "Topic: 0.021*\"test\" + 0.019*\"year\" + 0.018*\"world\" + 0.017*\"record\" + 0.016*\"win\" + 0.013*\"england\" + 0.012*\"aussi\" + 0.012*\"lead\" + 0.011*\"final\" + 0.011*\"concern\"\n",
      "\n",
      "Score: 0.020002495497465134\t \n",
      "Topic: 0.032*\"claim\" + 0.025*\"reject\" + 0.020*\"open\" + 0.017*\"coast\" + 0.017*\"take\" + 0.014*\"warn\" + 0.013*\"storm\" + 0.013*\"head\" + 0.012*\"gold\" + 0.012*\"threat\"\n"
     ]
    }
   ],
   "source": [
    "# Check which topic our test document belongs to using the LDA BOW\n",
    "\n",
    "# tup: -1*tup[1] means to sort descend and tuple (index, score)\n",
    "for index, score in sorted(lda_model[bow_corpus[4310]],key = lambda tup: -1* tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2 Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6447213292121887\t \n",
      "Topic: 0.011*\"price\" + 0.010*\"market\" + 0.008*\"bushfir\" + 0.007*\"rise\" + 0.007*\"nuclear\" + 0.006*\"plan\" + 0.006*\"govt\" + 0.006*\"rat\" + 0.005*\"bail\" + 0.005*\"council\"\n",
      "\n",
      "Score: 0.19524641335010529\t \n",
      "Topic: 0.016*\"water\" + 0.009*\"govt\" + 0.008*\"fund\" + 0.007*\"plan\" + 0.007*\"council\" + 0.007*\"boost\" + 0.006*\"rain\" + 0.006*\"urg\" + 0.006*\"coast\" + 0.006*\"drought\"\n",
      "\n",
      "Score: 0.02000492811203003\t \n",
      "Topic: 0.009*\"rudd\" + 0.008*\"govt\" + 0.007*\"develop\" + 0.007*\"guilti\" + 0.006*\"care\" + 0.006*\"climat\" + 0.006*\"chang\" + 0.006*\"council\" + 0.005*\"truck\" + 0.005*\"cancer\"\n",
      "\n",
      "Score: 0.020004484802484512\t \n",
      "Topic: 0.008*\"bird\" + 0.007*\"test\" + 0.006*\"england\" + 0.006*\"drug\" + 0.006*\"boat\" + 0.005*\"illeg\" + 0.005*\"kangaroo\" + 0.005*\"pakistan\" + 0.005*\"lanka\" + 0.005*\"polic\"\n",
      "\n",
      "Score: 0.02000434510409832\t \n",
      "Topic: 0.023*\"crash\" + 0.022*\"polic\" + 0.013*\"miss\" + 0.013*\"search\" + 0.012*\"investig\" + 0.010*\"die\" + 0.010*\"road\" + 0.009*\"fatal\" + 0.008*\"driver\" + 0.008*\"accid\"\n",
      "\n",
      "Score: 0.020004311576485634\t \n",
      "Topic: 0.008*\"hick\" + 0.007*\"farm\" + 0.007*\"grower\" + 0.007*\"speed\" + 0.007*\"doubt\" + 0.007*\"wind\" + 0.007*\"govt\" + 0.005*\"appoint\" + 0.005*\"merger\" + 0.005*\"downer\"\n",
      "\n",
      "Score: 0.020003968849778175\t \n",
      "Topic: 0.007*\"teacher\" + 0.006*\"strike\" + 0.006*\"union\" + 0.006*\"isra\" + 0.006*\"govt\" + 0.006*\"palestinian\" + 0.006*\"kill\" + 0.006*\"israel\" + 0.006*\"beazley\" + 0.005*\"concern\"\n",
      "\n",
      "Score: 0.020003601908683777\t \n",
      "Topic: 0.008*\"shortag\" + 0.007*\"retir\" + 0.005*\"murray\" + 0.005*\"staff\" + 0.005*\"season\" + 0.004*\"record\" + 0.004*\"villag\" + 0.004*\"success\" + 0.004*\"darl\" + 0.004*\"forc\"\n",
      "\n",
      "Score: 0.02000357210636139\t \n",
      "Topic: 0.027*\"closer\" + 0.017*\"charg\" + 0.014*\"court\" + 0.011*\"face\" + 0.011*\"jail\" + 0.010*\"assault\" + 0.010*\"polic\" + 0.009*\"murder\" + 0.009*\"kill\" + 0.009*\"blaze\"\n",
      "\n",
      "Score: 0.020003080368041992\t \n",
      "Topic: 0.008*\"open\" + 0.007*\"final\" + 0.007*\"troop\" + 0.006*\"tiger\" + 0.006*\"aussi\" + 0.006*\"win\" + 0.006*\"solomon\" + 0.006*\"australia\" + 0.005*\"uranium\" + 0.005*\"black\"\n"
     ]
    }
   ],
   "source": [
    "# Check which topic test document belongs to using the LDA TF-IDF model\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA BOW model performance:\n",
      "\n",
      "Score: 0.2749941647052765\t Topic: 0.020*\"forc\" + 0.019*\"lead\" + 0.015*\"win\" + 0.014*\"take\" + 0.014*\"world\"\n",
      "\n",
      "Score: 0.2749866247177124\t Topic: 0.018*\"open\" + 0.015*\"boost\" + 0.014*\"rain\" + 0.013*\"stand\" + 0.012*\"action\"\n",
      "\n",
      "Score: 0.2749808430671692\t Topic: 0.054*\"govt\" + 0.040*\"urg\" + 0.024*\"help\" + 0.023*\"call\" + 0.016*\"fund\"\n",
      "\n",
      "Score: 0.02500712126493454\t Topic: 0.023*\"closer\" + 0.018*\"talk\" + 0.016*\"market\" + 0.016*\"deal\" + 0.014*\"nuclear\"\n",
      "\n",
      "Score: 0.025006674230098724\t Topic: 0.032*\"council\" + 0.019*\"health\" + 0.019*\"minist\" + 0.019*\"servic\" + 0.015*\"opposit\"\n",
      "\n",
      "Score: 0.025006195530295372\t Topic: 0.038*\"crash\" + 0.026*\"investig\" + 0.023*\"polic\" + 0.017*\"die\" + 0.015*\"victim\"\n",
      "\n",
      "Score: 0.025004586204886436\t Topic: 0.037*\"charg\" + 0.032*\"court\" + 0.030*\"face\" + 0.020*\"jail\" + 0.020*\"accus\"\n",
      "\n",
      "Score: 0.025004586204886436\t Topic: 0.044*\"water\" + 0.032*\"plan\" + 0.018*\"council\" + 0.016*\"concern\" + 0.014*\"fear\"\n",
      "\n",
      "Score: 0.025004586204886436\t Topic: 0.047*\"polic\" + 0.029*\"kill\" + 0.026*\"miss\" + 0.024*\"attack\" + 0.023*\"warn\"\n",
      "\n",
      "Score: 0.025004586204886436\t Topic: 0.019*\"labor\" + 0.016*\"rise\" + 0.016*\"govt\" + 0.013*\"say\" + 0.012*\"road\"\n",
      "\n",
      "\n",
      "LDA TF-IDF model performance:\n",
      "\n",
      "Score: 0.5044487118721008\t Topic: 0.016*\"water\" + 0.009*\"govt\" + 0.008*\"fund\" + 0.007*\"plan\" + 0.007*\"council\"\n",
      "\n",
      "Score: 0.2955193817615509\t Topic: 0.007*\"teacher\" + 0.006*\"strike\" + 0.006*\"union\" + 0.006*\"isra\" + 0.006*\"govt\"\n",
      "\n",
      "Score: 0.02500556781888008\t Topic: 0.009*\"rudd\" + 0.008*\"govt\" + 0.007*\"develop\" + 0.007*\"guilti\" + 0.006*\"care\"\n",
      "\n",
      "Score: 0.025004571303725243\t Topic: 0.023*\"crash\" + 0.022*\"polic\" + 0.013*\"miss\" + 0.013*\"search\" + 0.012*\"investig\"\n",
      "\n",
      "Score: 0.025004524737596512\t Topic: 0.008*\"open\" + 0.007*\"final\" + 0.007*\"troop\" + 0.006*\"tiger\" + 0.006*\"aussi\"\n",
      "\n",
      "Score: 0.025004012510180473\t Topic: 0.008*\"hick\" + 0.007*\"farm\" + 0.007*\"grower\" + 0.007*\"speed\" + 0.007*\"doubt\"\n",
      "\n",
      "Score: 0.02500339411199093\t Topic: 0.008*\"bird\" + 0.007*\"test\" + 0.006*\"england\" + 0.006*\"drug\" + 0.006*\"boat\"\n",
      "\n",
      "Score: 0.025003299117088318\t Topic: 0.008*\"shortag\" + 0.007*\"retir\" + 0.005*\"murray\" + 0.005*\"staff\" + 0.005*\"season\"\n",
      "\n",
      "Score: 0.025003280490636826\t Topic: 0.011*\"price\" + 0.010*\"market\" + 0.008*\"bushfir\" + 0.007*\"rise\" + 0.007*\"nuclear\"\n",
      "\n",
      "Score: 0.02500326745212078\t Topic: 0.027*\"closer\" + 0.017*\"charg\" + 0.014*\"court\" + 0.011*\"face\" + 0.011*\"jail\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'My favoriate sports are running and swimming.'\n",
    "\n",
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "print('LDA BOW model performance:')\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))\n",
    "print('\\n')\n",
    "print('LDA TF-IDF model performance:')\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index,5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
